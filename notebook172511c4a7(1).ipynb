{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "notebook172511c4a7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1QuK4L_GjI6",
        "outputId": "77971988-77ce-4559-985d-91bccdacdbd8"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os\n",
        "import torch\n",
        "!pip install timm\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import cv2\n",
        "import os\n",
        "!pip install albumentations\n",
        "import albumentations as A\n",
        "\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "torch.cuda.is_available()\n",
        "torch.manual_seed(1337)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[?25l\r\u001b[K     |▉                               | 10 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20 kB 25.1 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 30 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 40 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 61 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████                          | 71 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |███████                         | 81 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 92 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 102 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 112 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 122 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 133 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 143 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 153 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 163 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 174 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 184 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 194 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 204 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 215 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 225 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 235 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 245 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 256 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 266 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 276 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 286 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 296 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 307 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 317 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 327 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 337 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 348 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 358 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 368 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 376 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.9.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.19.5)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.4.12\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.7/dist-packages (0.1.12)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from albumentations) (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.19.5)\n",
            "Collecting imgaug<0.2.7,>=0.2.5\n",
            "  Downloading imgaug-0.2.6.tar.gz (631 kB)\n",
            "\u001b[K     |████████████████████████████████| 631 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\n",
            "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (0.16.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (1.15.0)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (7.1.2)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (3.2.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.6.3)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.4.7)\n",
            "Building wheels for collected packages: imgaug\n",
            "  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imgaug: filename=imgaug-0.2.6-py3-none-any.whl size=654020 sha256=386a23aa1e2ede750662cc6759e46fad5a2c3b93139687050e0c7097ca5c0072\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/72/98/3ebfdba1069a9a8eaaa7ae7265cfd67d63ef0197aaee2e5f9c\n",
            "Successfully built imgaug\n",
            "Installing collected packages: imgaug\n",
            "  Attempting uninstall: imgaug\n",
            "    Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "Successfully installed imgaug-0.2.6\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fbc248a47f0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqtPapAVVViX"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "!unzip /content/drive/MyDrive/prepared_data\n",
        "! ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmtdLl-LmUVN"
      },
      "source": [
        "! diff /content/drive/MyDrive/train_giant.csv /content/images_train_resized/train_giant.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKKS-oO0GjJB",
        "outputId": "c6577d90-c7d5-4c74-fc2c-75d70c401db0"
      },
      "source": [
        "#data_anno_path = '/kaggle/input/hogweed-dataset/'\n",
        "data_anno_path = '/content/images_train_resized/'\n",
        "old_test = pd.read_csv(data_anno_path+\"train.csv\")\n",
        "df = pd.read_csv(data_anno_path+\"train_giant.csv\")#old_test.append(pd.read_csv(data_anno_path+\"train_small.csv\"))\n",
        "print(df.shape)\n",
        "df = df.drop(df[df.id == '4b58ef45756f7998b127952202a903e5'].index)\n",
        "df = df.drop(df[df.id == '5a4e59e53496bdfdca7afed12f3afd9a'].index)\n",
        "df = df.drop(df[df.id == '766e7669a5c7bb5f758acfd58ec05e0f'].index)\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16497, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16494, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oMGjAocGjJD",
        "outputId": "c2d55dd2-630d-4224-b7cf-023dea61f811"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, validation = train_test_split(df,\n",
        "                                     test_size=0.3,\n",
        "                                     random_state=1337,\n",
        "                                     shuffle=True,\n",
        "                                     stratify=df['has_hogweed'])\n",
        "\n",
        "print(len(train), len(validation), len(old_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11545 4949 311\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u47qFnhTZ449",
        "outputId": "972af56a-e993-4222-c413-da9057c370a0"
      },
      "source": [
        "\n",
        "for name in df['id']:\n",
        " image_path1 = os.path.join(data_anno_path + 'has_hogweed/', name)\n",
        " image_path2 = os.path.join(data_anno_path + 'no_hogweed/', name)\n",
        " image_path3 = os.path.join(data_anno_path + 'giant_hogweed/', name)\n",
        " image_path4 = os.path.join(data_anno_path + 'giant_no_hogweed/', name)\n",
        " img = cv2.imread(image_path1 + '.jpg')\n",
        " if str(type(img)) == \"<class 'NoneType'>\":\n",
        "   img = cv2.imread(image_path2 + '.jpg')\n",
        " if str(type(img)) == \"<class 'NoneType'>\":\n",
        "   img = cv2.imread(image_path3 + '.jpg')\n",
        " if str(type(img)) == \"<class 'NoneType'>\":\n",
        "   img = cv2.imread(image_path4 + '.jpg')  \n",
        " if str(type(img)) == \"<class 'NoneType'>\":\n",
        "     print(name)\n",
        "print('done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUY6qrfIGjJD"
      },
      "source": [
        "# DATALOADER + AUGMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBZKi35cGjJF"
      },
      "source": [
        "class ClassifficationDataset(Dataset):\n",
        "    def __init__(self, df, data_path, transrofmation=None, test=False):\n",
        "        self.data_anno = df\n",
        "        #self.data_path = data_path + 'images_small' if not test else data_path + 'images_test_resized/images_test_resized/unknown/'\n",
        "        self.data_path = data_path\n",
        "        self.transform = transrofmation\n",
        "        self.test = test\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        image_name = self.data_anno['id'].iloc[idx]\n",
        "        if not self.test:\n",
        "            label = self.data_anno['has_hogweed'].iloc[idx]\n",
        "        if not self.test:\n",
        "            image_path1 = os.path.join(self.data_path + 'has_hogweed/', image_name)\n",
        "            image_path2 = os.path.join(self.data_path + 'no_hogweed/', image_name)\n",
        "            image_path3 = os.path.join(self.data_path + 'giant_hogweed/', image_name)\n",
        "            image_path4 = os.path.join(self.data_path + 'giant_no_hogweed/', image_name)\n",
        "            img = cv2.imread(image_path1 + '.jpg')\n",
        "            if str(type(img)) == \"<class 'NoneType'>\":\n",
        "                img = cv2.imread(image_path2 + '.jpg')\n",
        "            if str(type(img)) == \"<class 'NoneType'>\":\n",
        "                img = cv2.imread(image_path3 + '.jpg')\n",
        "            if str(type(img)) == \"<class 'NoneType'>\":\n",
        "                img = cv2.imread(image_path4 + '.jpg')\n",
        "        else:\n",
        "            image_path = os.path.join(self.data_path, image_name)\n",
        "            img = cv2.imread(image_path + '.jpg')\n",
        "            \n",
        "        if self.transform is not None:\n",
        "            img = self.transform(image=img)[\"image\"]\n",
        "\n",
        "        img = self.preprocess(img)\n",
        "        if self.test:\n",
        "            return torch.from_numpy(img).float()\n",
        "        else:\n",
        "            return torch.from_numpy(img).float(), label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_anno)\n",
        "\n",
        "    def preprocess(self, img):\n",
        "\n",
        "        img = cv2.resize(img, (384, 384))\n",
        "        img = img.transpose(2, 0, 1)\n",
        "        img = (img - 127.5) / 128\n",
        "\n",
        "        return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yc5jDL-KGjJH"
      },
      "source": [
        "transformator = A.Compose([\n",
        "#   A.Blur(p = 0.5),\n",
        "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=10, p=0.3),\n",
        "#    A.HorizontalFlip(p=0.3),\n",
        "#    A.ToGray(p=1)\n",
        "    A.RandomBrightnessContrast(contrast_limit = 0.1, brightness_limit=0.1,p=0.5),\n",
        "#    A.RandomFog(p=0.3),\n",
        "#   A.ToGray(p=0.1)\n",
        "])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l13xIE1wGjJJ",
        "outputId": "97246df7-74d6-43c4-8f42-4c9af670a6fe"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "train_dataset = ClassifficationDataset(train,\n",
        "                                      data_anno_path, \n",
        "                                      transrofmation=transformator)\n",
        "\n",
        "valid_dataset = ClassifficationDataset(validation,\n",
        "                                      data_anno_path, \n",
        "                                      transrofmation=None)\n",
        "\n",
        "old_dataset = ClassifficationDataset(old_test,\n",
        "                                      data_anno_path, \n",
        "                                      transrofmation=None)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=15, shuffle=True,\n",
        "                              num_workers=8)\n",
        "\n",
        "val_dataloader = DataLoader(valid_dataset, batch_size=15, shuffle=False,\n",
        "                            num_workers=8)\n",
        "\n",
        "old_dataloader = DataLoader(old_dataset, batch_size=15, shuffle=False, \n",
        "                            num_workers=8)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rinEiahGe6kp",
        "outputId": "6341d7a7-0859-4382-a22d-c45d9fb0dc37"
      },
      "source": [
        "train_dataloader.__iter__().__next__()[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([15, 3, 384, 384])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ejPwONGGjJK"
      },
      "source": [
        "# MODEL GENERATING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M30pkbgnGjJL",
        "outputId": "d68445c7-b4ac-43fc-8976-77f739d78c7b"
      },
      "source": [
        "\n",
        "model = timm.create_model('efficientnet_b1', pretrained=True)\n",
        "#model.fc = nn.Sequential(nn.Linear(2048, 2))\n",
        "#model.head = nn.Linear(384, 6)\n",
        "model.classifier = nn.Sequential(nn.Linear(1280, 2))\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EfficientNet(\n",
            "  (conv_stem): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (act1): SiLU(inplace=True)\n",
            "  (blocks): Sequential(\n",
            "    (0): Sequential(\n",
            "      (0): DepthwiseSeparableConv(\n",
            "        (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act1): SiLU(inplace=True)\n",
            "        (se): SqueezeExcite(\n",
            "          (conv_reduce): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act1): SiLU(inplace=True)\n",
            "          (conv_expand): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (gate): Sigmoid()\n",
            "        )\n",
            "        (conv_pw): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act2): Identity()\n",
            "      )\n",
            "      (1): DepthwiseSeparableConv(\n",
            "        (conv_dw): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
            "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act1): SiLU(inplace=True)\n",
            "        (se): SqueezeExcite(\n",
            "          (conv_reduce): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act1): SiLU(inplace=True)\n",
            "          (conv_expand): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (gate): Sigmoid()\n",
            "        )\n",
            "        (conv_pw): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act2): Identity()\n",
            "      )\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): InvertedResidual(\n",
            "        (conv_pw): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act1): SiLU(inplace=True)\n",
            "        (conv_dw): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
            "        (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act2): SiLU(inplace=True)\n",
            "        (se): SqueezeExcite(\n",
            "          (conv_reduce): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act1): SiLU(inplace=True)\n",
            "          (conv_expand): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (gate): Sigmoid()\n",
            "        )\n",
            "        (conv_pwl): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): InvertedResidual(\n",
            "        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act1): SiLU(inplace=True)\n",
            "        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
            "        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act2): SiLU(inplace=True)\n",
            "        (se): SqueezeExcite(\n",
            "          (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act1): SiLU(inplace=True)\n",
            "          (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (gate): Sigmoid()\n",
            "        )\n",
            "        (conv_pwl): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (2): InvertedResidual(\n",
            "        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act1): SiLU(inplace=True)\n",
            "        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
            "        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act2): SiLU(inplace=True)\n",
            "        (se): SqueezeExcite(\n",
            "          (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act1): SiLU(inplace=True)\n",
            "          (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (gate): Sigmoid()\n",
            "        )\n",
            "        (conv_pwl): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (2): Sequential(\n",
            "      (0): InvertedResidual(\n",
            "        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act1): SiLU(inplace=True)\n",
            "        (conv_dw): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
            "        (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act2): SiLU(inplace=True)\n",
            "        (se): SqueezeExcite(\n",
            "          (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act1): SiLU(inplace=True)\n",
            "          (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (gate): Sigmoid()\n",
            "        )\n",
            "        (conv_pwl): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): InvertedResidual(\n",
            "        (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act1): SiLU(inplace=True)\n",
            "        (conv_dw): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
            "        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act2): SiLU(inplace=True)\n",
            "        (se): SqueezeExcite(\n",
            "          (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act1): SiLU(inplace=True)\n",
            "          (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (gate): Sigmoid()\n",
            "        )\n",
            "        (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (2): InvertedResidual(\n",
            "        (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act1): SiLU(inplace=True)\n",
            "        (conv_dw): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
            "        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act2): SiLU(inplace=True)\n",
            "        (se): SqueezeExcite(\n",
            "          (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act1): SiLU(inplace=True)\n",
            "          (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (gate): Sigmoid()\n",
            "        )\n",
            "        (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (3): Sequential(\n",
            "      (0): InvertedResidual(\n",
            "        (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act1): SiLU(inplace=True)\n",
            "        (conv_dw): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
            "        (bn2): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act2): SiLU(inplace=True)\n",
            "        (se): SqueezeExcite(\n",
            "          (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act1): SiLU(inplace=True)\n",
            "          (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (gate): Sigmoid()\n",
            "        )\n",
            "        (conv_pwl): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): InvertedResidual(\n",
            "        (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act1): SiLU(inplace=True)\n",
            "        (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
            "        (bn2): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act2): SiLU(inplace=True)\n",
            "        (se): SqueezeExcite(\n",
            "          (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act1): SiLU(inplace=True)\n",
            "          (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (gate): Sigmoid()\n",
            "        )\n",
            "        (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (2): InvertedResidual(\n",
            "        (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act1): SiLU(inplace=True)\n",
            "        (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
            "        (bn2): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act2): SiLU(inplace=True)\n",
            "        (se): SqueezeExcite(\n",
            "          (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act1): SiLU(inplace=True)\n",
            "          (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (gate): Sigmoid()\n",
            "        )\n",
            "        (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (3): InvertedResidual(\n",
            "        (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act1): SiLU(inplace=True)\n",
            "        (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
            "        (bn2): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act2): SiLU(inplace=True)\n",
            "        (se): SqueezeExcite(\n",
            "          (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act1): SiLU(inplace=True)\n",
            "          (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (gate): Sigmoid()\n",
            "        )\n",
            "        (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (4): Sequential(\n",
            "      (0): InvertedResidual(\n",
            "        (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act1): SiLU(inplace=True)\n",
            "        (conv_dw): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
            "        (bn2): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act2): SiLU(inplace=True)\n",
            "        (se): SqueezeExcite(\n",
            "          (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act1): SiLU(inplace=True)\n",
            "          (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (gate): Sigmoid()\n",
            "        )\n",
            "        (conv_pwl): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): InvertedResidual(\n",
            "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act1): SiLU(inplace=True)\n",
            "        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
            "        (bn2): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act2): SiLU(inplace=True)\n",
            "        (se): SqueezeExcite(\n",
            "          (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act1): SiLU(inplace=True)\n",
            "          (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (gate): Sigmoid()\n",
            "        )\n",
            "        (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (2): InvertedResidual(\n",
            "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act1): SiLU(inplace=True)\n",
            "        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
            "        (bn2): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act2): SiLU(inplace=True)\n",
            "        (se): SqueezeExcite(\n",
            "          (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act1): SiLU(inplace=True)\n",
            "          (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (gate): Sigmoid()\n",
            "        )\n",
            "        (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (3): InvertedResidual(\n",
            "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act1): SiLU(inplace=True)\n",
            "        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
            "        (bn2): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act2): SiLU(inplace=True)\n",
            "        (se): SqueezeExcite(\n",
            "          (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act1): SiLU(inplace=True)\n",
            "          (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (gate): Sigmoid()\n",
            "        )\n",
            "        (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (5): Sequential(\n",
            "      (0): InvertedResidual(\n",
            "        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act1): SiLU(inplace=True)\n",
            "        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
            "        (bn2): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act2): SiLU(inplace=True)\n",
            "        (se): SqueezeExcite(\n",
            "          (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act1): SiLU(inplace=True)\n",
            "          (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (gate): Sigmoid()\n",
            "        )\n",
            "        (conv_pwl): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): InvertedResidual(\n",
            "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act1): SiLU(inplace=True)\n",
            "        (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
            "        (bn2): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act2): SiLU(inplace=True)\n",
            "        (se): SqueezeExcite(\n",
            "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act1): SiLU(inplace=True)\n",
            "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (gate): Sigmoid()\n",
            "        )\n",
            "        (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (2): InvertedResidual(\n",
            "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act1): SiLU(inplace=True)\n",
            "        (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
            "        (bn2): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act2): SiLU(inplace=True)\n",
            "        (se): SqueezeExcite(\n",
            "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act1): SiLU(inplace=True)\n",
            "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (gate): Sigmoid()\n",
            "        )\n",
            "        (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (3): InvertedResidual(\n",
            "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act1): SiLU(inplace=True)\n",
            "        (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
            "        (bn2): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act2): SiLU(inplace=True)\n",
            "        (se): SqueezeExcite(\n",
            "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act1): SiLU(inplace=True)\n",
            "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (gate): Sigmoid()\n",
            "        )\n",
            "        (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (4): InvertedResidual(\n",
            "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act1): SiLU(inplace=True)\n",
            "        (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
            "        (bn2): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act2): SiLU(inplace=True)\n",
            "        (se): SqueezeExcite(\n",
            "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act1): SiLU(inplace=True)\n",
            "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (gate): Sigmoid()\n",
            "        )\n",
            "        (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (6): Sequential(\n",
            "      (0): InvertedResidual(\n",
            "        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act1): SiLU(inplace=True)\n",
            "        (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
            "        (bn2): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act2): SiLU(inplace=True)\n",
            "        (se): SqueezeExcite(\n",
            "          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act1): SiLU(inplace=True)\n",
            "          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (gate): Sigmoid()\n",
            "        )\n",
            "        (conv_pwl): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): InvertedResidual(\n",
            "        (conv_pw): Conv2d(320, 1920, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act1): SiLU(inplace=True)\n",
            "        (conv_dw): Conv2d(1920, 1920, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1920, bias=False)\n",
            "        (bn2): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (act2): SiLU(inplace=True)\n",
            "        (se): SqueezeExcite(\n",
            "          (conv_reduce): Conv2d(1920, 80, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (act1): SiLU(inplace=True)\n",
            "          (conv_expand): Conv2d(80, 1920, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (gate): Sigmoid()\n",
            "        )\n",
            "        (conv_pwl): Conv2d(1920, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (conv_head): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "  (bn2): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (act2): SiLU(inplace=True)\n",
            "  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=1280, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1whqGWYGjJM"
      },
      "source": [
        "#for p in model.parameters():\n",
        "#    p.require_grad = False\n",
        "#model.classifier = nn.Sequential(nn.Linear(1280, 2))\n",
        "#model.fc = nn.Sequential(nn.Linear(2048, 2))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PzzU_5AGjJN"
      },
      "source": [
        "model.cuda()\n",
        "pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeB-qjPuGjJO"
      },
      "source": [
        "# OPTIMIZER, SCHEDULER, ETC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cef2m8NUGjJP"
      },
      "source": [
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group[\"lr\"]\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr_sched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 2, eta_min = 1e-5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOdDJsciGjJQ"
      },
      "source": [
        "# TRAIN + VAL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "UMHDIwDWGjJQ",
        "outputId": "5b2febfb-4db4-469d-d2ae-a6341882711a"
      },
      "source": [
        "num_epoch = 2\n",
        "print_frec = 30\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "    lr_sched.step(epoch)\n",
        "    total_loss = 0\n",
        "    corrects = 0\n",
        "    num_samples = 0\n",
        "    model.train()\n",
        "\n",
        "    for i, (img, label) in enumerate(train_dataloader):\n",
        "        i += 1\n",
        "        img = img.cuda()\n",
        "        label = label.cuda()\n",
        "        out = model(img)\n",
        "        optimizer.zero_grad()\n",
        "        #with torch.cuda.amp.autocast():\n",
        "        #    loss = criterion(out, label)\n",
        "        #scaler.scale(loss).backward()\n",
        "        #scaler.step(optimizer)\n",
        "        loss = criterion(out, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        num_samples += img.size(0)\n",
        "        _, preds = torch.max(out, 1)\n",
        "        corrects += torch.sum(preds == label.data).item()\n",
        "        #scaler.update()\n",
        "\n",
        "        # if i % print_frec == 0:\n",
        "        #     print('Loss:', total_loss / i, 'Train_accuracy:', corrects / num_samples)\n",
        "\n",
        "    print('Epoch:',epoch,'Train_Loss:', total_loss / i, 'Train_accuracy:', corrects / num_samples)\n",
        "\n",
        "    model.eval()\n",
        "    val_correct = 0\n",
        "    val_loss = 0\n",
        "    num_samples = 0\n",
        "    for i, (img, label) in enumerate(val_dataloader):\n",
        "        img = img.cuda()\n",
        "        label = label.cuda()\n",
        "        out = model(img)\n",
        "        #with torch.cuda.amp.autocast():\n",
        "        #    loss = criterion(out, label)\n",
        "        loss = criterion(out, label)\n",
        "        val_loss += loss.item()\n",
        "        num_samples += img.size(0)\n",
        "        torch.save(model.state_dict(), '/content/drive/MyDrive/best_model.pth')\n",
        "\n",
        "        _, preds = torch.max(out, 1)\n",
        "        val_correct += torch.sum(preds == label.data).item()\n",
        "    print('Val_Loss:', val_loss / i, 'Val_accuracy:', val_correct / num_samples,'LR: ', get_lr(optimizer))\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/best_model.pth')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-4145d41c6ff8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#with torch.cuda.amp.autocast():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/timm/models/efficientnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_rate\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/timm/models/efficientnet.py\u001b[0m in \u001b[0;36mforward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_stem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 440\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 11.17 GiB total capacity; 10.36 GiB already allocated; 37.81 MiB free; 10.62 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEjsRVjbGjJR"
      },
      "source": [
        "# Check old train accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smz4ftUwGjJS"
      },
      "source": [
        "torch.save(model.state_dict(), 'best_model.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7so8aS7ZGjJU"
      },
      "source": [
        "num_epoch = 1\n",
        "print_frec = 30\n",
        "\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/best_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "    val_correct = 0\n",
        "    val_loss = 0\n",
        "    num_samples = 0\n",
        "    for i, (img, label) in enumerate(old_dataloader):\n",
        "        img = img.cuda()\n",
        "        label = label.cuda()\n",
        "        out = model(img)\n",
        "        #with torch.cuda.amp.autocast():\n",
        "        #    loss = criterion(out, label)\n",
        "        loss = criterion(out, label)\n",
        "        val_loss += loss.item()\n",
        "        num_samples += img.size(0)\n",
        "\n",
        "        _, preds = torch.max(out, 1)\n",
        "        val_correct += torch.sum(preds == label.data).item()\n",
        "    print('Val_Loss:', val_loss / i, 'Val_accuracy:', val_correct / num_samples)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ3lPLR-GjJV"
      },
      "source": [
        "# FINE-TUNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53GQ86zKGjJV"
      },
      "source": [
        "preparing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JEFCqOGGjJV"
      },
      "source": [
        "model.load_state_dict(torch.load('/content/drive/MyDrive/best_model.pth'))\n",
        "#for p in model.parameters():\n",
        "#    p.require_grad = False\n",
        "\n",
        "#model.classifier = nn.Sequential(nn.Linear(1280, 2))\n",
        "model.cuda()\n",
        "pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS5ugc2kGjJW"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "tune, val = train_test_split(old_test,\n",
        "                                     test_size=0.3,\n",
        "                                     random_state=1337,\n",
        "                                     shuffle=True,\n",
        "                                     stratify=old_test['has_hogweed'])\n",
        "\n",
        "print(len(tune), len(val))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1ptNmBqGjJW"
      },
      "source": [
        "sum(val['has_hogweed'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFFj9wHwGjJX"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr_sched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 3, eta_min = 1e-5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxZrAcSZGjJX"
      },
      "source": [
        "tune_dataset = ClassifficationDataset(tune,\n",
        "                                      data_anno_path,test=False)\n",
        "vaal_dataset = ClassifficationDataset(tune,\n",
        "                                      data_anno_path,test=False)\n",
        "\n",
        "\n",
        "tune_dataloader = DataLoader(tune_dataset, batch_size=15, shuffle=True,\n",
        "                              num_workers=4)\n",
        "vaal_dataloader = DataLoader(vaal_dataset, batch_size=15, shuffle=False,\n",
        "                              num_workers=4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiJe6SkjGjJX"
      },
      "source": [
        "num_epoch = 3\n",
        "print_frec = 30\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "    lr_sched.step(epoch)\n",
        "    total_loss = 0\n",
        "    corrects = 0\n",
        "    num_samples = 0\n",
        "    model.train()\n",
        "\n",
        "    for i, (img, label) in enumerate(tune_dataloader):\n",
        "        i += 1\n",
        "        img = img.cuda()\n",
        "        label = label.cuda()\n",
        "        out = model(img)\n",
        "        optimizer.zero_grad()\n",
        "        #with torch.cuda.amp.autocast():\n",
        "        #    loss = criterion(out, label)\n",
        "        #scaler.scale(loss).backward()\n",
        "        #scaler.step(optimizer)\n",
        "        loss = criterion(out, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        num_samples += img.size(0)\n",
        "        _, preds = torch.max(out, 1)\n",
        "        corrects += torch.sum(preds == label.data).item()\n",
        "        #scaler.update()\n",
        "\n",
        "        # if i % print_frec == 0:\n",
        "        #     print('Loss:', total_loss / i, 'Train_accuracy:', corrects / num_samples)\n",
        "\n",
        "    print('Epoch:',epoch,'Train_Loss:', total_loss / i, 'Train_accuracy:', corrects / num_samples)\n",
        "\n",
        "    model.eval()\n",
        "    val_correct = 0\n",
        "    val_loss = 0\n",
        "    num_samples = 0\n",
        "    for i, (img, label) in enumerate(vaal_dataloader):\n",
        "        img = img.cuda()\n",
        "        label = label.cuda()\n",
        "        out = model(img)\n",
        "        #with torch.cuda.amp.autocast():\n",
        "        #    loss = criterion(out, label)\n",
        "        loss = criterion(out, label)\n",
        "        val_loss += loss.item()\n",
        "        num_samples += img.size(0)\n",
        "\n",
        "        _, preds = torch.max(out, 1)\n",
        "        val_correct += torch.sum(preds == label.data).item()\n",
        "    print('Val_Loss:', val_loss / i, 'Val_accuracy:', val_correct / num_samples,'LR: ', get_lr(optimizer))\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), 'best_model.pth')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74O6JL0NGjJX"
      },
      "source": [
        "# SUBMISSION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PleLcWEKGjJY"
      },
      "source": [
        "torch.cuda.empty_cache() \n",
        "\n",
        "test_data_anno_path = '/content/images_test_resized/unknown'\n",
        "test = pd.read_csv(test_data_anno_path + '/sample_submission.csv')\n",
        "#print(test.head())\n",
        "test_dataset = ClassifficationDataset(test,\n",
        "                                      '/content/images_test_resized/unknown',test=True)\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False,\n",
        "                              num_workers=4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTA0QksEGjJY"
      },
      "source": [
        "\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "model.eval()\n",
        "predicts = []\n",
        "for img in test_dataloader:\n",
        "    img = img.cuda()\n",
        "\n",
        "    out = model(img)\n",
        "\n",
        "    _, preds = torch.max(out, 1)\n",
        "#     print(preds)\n",
        "    predicts.append(preds.detach().cpu().numpy())\n",
        "predicts = np.array(predicts)\n",
        "predicts = np.concatenate(predicts)\n",
        "test['has_hogweed'] = predicts\n",
        "print(test)\n",
        "test.to_csv('my_submission.csv', index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}